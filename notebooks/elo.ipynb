{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NCAABB Elo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.consts import ESPNSportTypes, START_SEASONS, SEASON_GROUPS\n",
    "from src.utils import find_year_for_season, create_dataframe, put_dataframe, get_dataframe, df_rename_fold\n",
    "from src.sport import ESPNSport\n",
    "from src.event import ESPNEventsAPI\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from src.consts import ESPNSportTypes, SEASON_GROUPS, ELO_HYPERPARAMETERS\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, brier_score_loss, log_loss\n",
    "from sklearn.metrics import accuracy_score, precision_score,recall_score,f1_score,roc_auc_score\n",
    "import datetime\n",
    "def _classification_evaluation(y_true, y_pred, k=20,hfa=68):\n",
    "    metadata_dict = {}\n",
    "    metadata_dict['created_at'] = datetime.datetime.now().strftime('%m-%d-%Y')\n",
    "    metadata_dict['sample_size'] = len(y_true)\n",
    "    metadata_dict['k'] = k\n",
    "    metadata_dict['hfa'] = hfa\n",
    "\n",
    "    y_true = np.array(y_true).ravel()\n",
    "    if set(y_true) == {0,1} or len(list(set(y_true))) <= 2:\n",
    "        # Binary classification\n",
    "        y_true_binary = y_true.astype(bool)\n",
    "        brier_score = brier_score_loss(y_true_binary, y_pred)\n",
    "        y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "        accuracy = accuracy_score(y_true_binary, y_pred_binary)\n",
    "        precision = precision_score(y_true_binary, y_pred_binary)\n",
    "        recall = recall_score(y_true_binary, y_pred_binary)\n",
    "        f1 = f1_score(y_true_binary, y_pred_binary)\n",
    "        if len(list(set(y_true))) < 2:\n",
    "            roc_auc = None\n",
    "            log_loss_score = None\n",
    "        else:\n",
    "            roc_auc = roc_auc_score(y_true_binary, y_pred_binary)\n",
    "            log_loss_score = log_loss(y_true_binary, y_pred)\n",
    "    else:\n",
    "        # Multiclass classification\n",
    "        y_true_multiclass = y_true\n",
    "        log_loss_score = log_loss(y_true_multiclass, y_pred)\n",
    "        y_pred_multiclass = np.argmax(y_pred, axis=1)\n",
    "        accuracy = accuracy_score(y_true_multiclass, y_pred_multiclass)\n",
    "        precision = precision_score(y_true_multiclass, y_pred_multiclass, average='weighted')\n",
    "        recall = recall_score(y_true_multiclass, y_pred_multiclass, average='weighted')\n",
    "        f1 = f1_score(y_true_multiclass, y_pred_multiclass, average='weighted')\n",
    "        roc_auc = None\n",
    "        brier_score = None\n",
    "\n",
    "    metrics_dict = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'roc_auc': roc_auc,\n",
    "        'brier_score': brier_score,\n",
    "        'log_loss_score': log_loss_score\n",
    "    }\n",
    "\n",
    "    metrics = []\n",
    "    for val, k in metrics_dict.items():\n",
    "        metrics.append(\n",
    "            {**metadata_dict, **{'metric_name': val, 'value': k}}\n",
    "        )\n",
    "    metadata_df = pd.DataFrame(metrics)\n",
    "    return metadata_df\n",
    "## If its refresh pull whole thing\n",
    "\n",
    "## If its upsert get the latest seasons in the fs and determine active sports that need pulling\n",
    "df = pd.DataFrame()\n",
    "for sport in [ESPNSportTypes.COLLEGE_BASKETBALL]:\n",
    "    current_season = find_year_for_season(sport)\n",
    "    seasons = list(range(START_SEASONS[sport], current_season +1))\n",
    "    for season in seasons:\n",
    "        fs_df = get_dataframe(f'./data/events/{sport.value}/{season}.parquet')\n",
    "        df = pd.concat([df,fs_df],ignore_index=True)\n",
    "\n",
    "folded_df = df_rename_fold(df[['season','home_team_name','home_team_score','away_team_name','away_team_score']],'away_','home_')\n",
    "num_games = folded_df.groupby(['team_name','season']).agg({\n",
    "    'team_score':'count'\n",
    "})\n",
    "num_games = num_games.reset_index().groupby('season')['team_score'].mean().mean()\n",
    "avg_score = folded_df.groupby(['team_name','season']).agg({\n",
    "    'team_score':'mean'\n",
    "})\n",
    "avg_score = avg_score.reset_index().groupby('season')['team_score'].mean().mean()\n",
    "\n",
    "\n",
    "hw = df.loc[df.neutral_site == 0].copy()\n",
    "hw['home_is_winner'] = hw['home_team_score'] > hw['away_team_score']\n",
    "hw = hw.groupby(['season']).agg({'home_is_winner':'sum','id':'count'}).reset_index()\n",
    "hw['perc'] = hw['home_is_winner'] / hw['id']\n",
    "hw = hw['perc'].mean()\n",
    "print(f'Averge Number of Games per Season: {round(num_games, 2)}')\n",
    "print(f'Averge Score per Season: {round(avg_score, 2)}')\n",
    "print(f'Averge Home Team Win Percentage per Season: {round(hw, 2)}')\n",
    "from src.elo import EloRunner\n",
    "elo_cols = ['str_event_id', 'season', 'date','neutral_site',  'home_team_id', 'home_team_score', 'away_team_id','away_team_score']\n",
    "\n",
    "## Default HPO \n",
    "k = int(num_games * 2) + 2\n",
    "hfa = int(hw * 100) + 2\n",
    "\n",
    "k = 25\n",
    "hfa = 75\n",
    "\n",
    "k = ELO_HYPERPARAMETERS[sport]['k']\n",
    "hfa = ELO_HYPERPARAMETERS[sport]['hfa']\n",
    "\n",
    "er = EloRunner(\n",
    "    df=df[elo_cols].rename(columns={'home_team_id':'home_team_name','away_team_id':'away_team_name'}),\n",
    "    allow_future=True,\n",
    "    k=k,\n",
    "    mean_elo=1505,\n",
    "    home_field_advantage=hfa,\n",
    "    width=800,\n",
    "    preloaded_elos=None\n",
    ")\n",
    "\n",
    "elo_df = er.run_to_date()\n",
    "elo_df = elo_df.rename(columns={'home_team_name':'home_team_id','away_team_name':'away_team_id'})\n",
    "elo_df = pd.merge(elo_df,df[['id','str_event_id','home_team_name','away_team_name','is_postseason','tournament_id','is_finished','datetime']],on=['str_event_id'])\n",
    "elo_df['result'] = elo_df['home_team_score'] > elo_df['away_team_score']\n",
    "#elo_df['error'] = elo_df['home_elo_prob'].round(2) - elo_df['result']\n",
    "#elo_df['sq_error'] = (elo_df['error']) **2\n",
    "#elo_df['points'] = 25 - (100 * elo_df['error'])\n",
    "#elo_df['elo_correct'] = elo_df['error'] < 0.5\n",
    "\n",
    "eval_df = elo_df.loc[((elo_df.is_finished == 1)&(elo_df.season >= START_SEASONS[sport] + 2))].copy()\n",
    "\n",
    "eval_df['error'] = eval_df['home_elo_prob'].round(2) - eval_df['result']\n",
    "eval_df['sq_error'] = (eval_df['error']) **2\n",
    "eval_df['points'] = 25 - (100 * eval_df['error'])\n",
    "\n",
    "y_true = eval_df['result']\n",
    "y_pred = eval_df['home_elo_prob']\n",
    "points = eval_df['points'].sum() / eval_df.shape[0]\n",
    "print(f\"Model Score: {round(points,2)}\")\n",
    "\n",
    "report = _classification_evaluation(y_true, y_pred, k=k,hfa=hfa)\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.steps.elo_runner import run_elo_for_sport\n",
    "run_elo_for_sport('./data/events','./data/elo', ESPNSportTypes.COLLEGE_BASKETBALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elo_df2 = pd.DataFrame()\n",
    "for sport in [ESPNSportTypes.COLLEGE_BASKETBALL]:\n",
    "    current_season = find_year_for_season(sport)\n",
    "    seasons = list(range(START_SEASONS[sport], current_season +1))\n",
    "    for season in seasons:\n",
    "        fs_df = get_dataframe(f'./data/elo/{sport.value}/{season}.parquet')\n",
    "        elo_df2 = pd.concat([elo_df2,fs_df],ignore_index=True)\n",
    "elo_df2['result'] = elo_df2['home_team_score'] > elo_df2['away_team_score']\n",
    "#elo_df['error'] = elo_df['home_elo_prob'].round(2) - elo_df['result']\n",
    "#elo_df['sq_error'] = (elo_df['error']) **2\n",
    "#elo_df['points'] = 25 - (100 * elo_df['error'])\n",
    "#elo_df['elo_correct'] = elo_df['error'] < 0.5\n",
    "\n",
    "eval_df2 = elo_df2.loc[((elo_df2.is_finished == 1)&(elo_df2.season >= START_SEASONS[sport] + 2))].copy()\n",
    "\n",
    "eval_df2['error'] = eval_df2['home_elo_prob'].round(2) - eval_df2['result']\n",
    "eval_df2['sq_error'] = (eval_df2['error']) **2\n",
    "eval_df2['points'] = 25 - (100 * eval_df2['error'])\n",
    "\n",
    "y_true = eval_df2['result']\n",
    "y_pred = eval_df2['home_elo_prob']\n",
    "points = eval_df2['points'].sum() / eval_df2.shape[0]\n",
    "print(f\"Model Score: {round(points,2)}\")\n",
    "\n",
    "report2 = _classification_evaluation(y_true, y_pred, k=k,hfa=hfa)\n",
    "report2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folded_elo_df = df_rename_fold(elo_df.loc[elo_df.is_finished==1][['season','datetime','home_team_name','away_team_name','home_team_id','away_team_id','home_elo_pre','away_elo_pre','home_elo_post','away_elo_post']],'away_','home_').sort_values('datetime')\n",
    "folded_elo_df.groupby('team_id').nth(-1).sort_values(['elo_post'],ascending=False)[0:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.elo import EloRunner\n",
    "elo_cols = ['str_event_id', 'season', 'date','neutral_site',  'home_team_id', 'home_team_score', 'away_team_id','away_team_score']\n",
    "\n",
    "reports = pd.DataFrame()\n",
    "for k in [10,15, 20, 25, 30]:\n",
    "    for hfa in [55, 60, 65, 70, 75]:\n",
    "        er = EloRunner(\n",
    "            df=df[elo_cols].rename(columns={'home_team_id':'home_team_name','away_team_id':'away_team_name'}),\n",
    "            allow_future=True,\n",
    "            k=k,\n",
    "            mean_elo=1505,\n",
    "            home_field_advantage=hfa,\n",
    "            width=800,\n",
    "            preloaded_elos=None\n",
    "        )\n",
    "\n",
    "        elo_df = er.run_to_date()\n",
    "        elo_df = elo_df.rename(columns={'home_team_name':'home_team_id','away_team_name':'away_team_id'})\n",
    "        elo_df = pd.merge(elo_df,df[['id','str_event_id','home_team_name','away_team_name','is_postseason','tournament_id','is_finished','datetime']],on=['str_event_id'])\n",
    "        elo_df['result'] = elo_df['home_team_score'] > elo_df['away_team_score']\n",
    "        eval_df = elo_df.loc[((elo_df.is_finished == 1)&(elo_df.season >= START_SEASONS[sport] + 2))].copy()\n",
    "\n",
    "        y_true = eval_df['result']\n",
    "        y_pred = eval_df['home_elo_prob']\n",
    "        report = _classification_evaluation(y_true, y_pred, k=k,hfa=hfa)\n",
    "        reports = pd.concat([reports, report], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports.loc[reports['metric_name'] == 'accuracy'].sort_values(['value'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
